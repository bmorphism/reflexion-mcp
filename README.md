# Reflexion Thinking MCP Server

[![Reflexion Thinking MCP Server](https://img.shields.io/badge/MCP%20Server-%40aquarius--wing%2Freflexion--thinking--mcp-blue)](https://glama.ai/mcp/servers/@aquarius-wing/reflexion-thinking-mcp) <!-- Replace with actual badge URL once available -->

This server implements the **Reflexion** methodology (Shinn et al., 2023) as a Model Context Protocol (MCP) tool. It enables iterative refinement of outputs through a structured process involving an **Actor**, an **Evaluator**, and **Self-Reflection**, augmented by an episodic **memory** of past reflections. This approach uses verbal reinforcement to guide an agent towards better performance on a given task.

## Features

- **Iterative Refinement:** Improves outputs through cycles of generation, evaluation, and self-reflection.
- **Verbal Reinforcement:** Uses natural language reflections to guide learning and improvement.
- **Modular Components:** Comprises distinct Actor, Evaluator, and Self-Reflection capabilities, orchestrated by the tool.
- **Episodic Memory:** Maintains a short-term memory (default depth: 3) of recent reflections to inform subsequent trials.
- **Structured Learning:** Follows a defined process (Actor -> Evaluator -> Self-Reflection) for each trial.
- **Actionable Feedback Loop:** Generates insights that the Actor (via an LLM) can use to modify behavior/output in future trials.

## Use Cases

- Iteratively solving complex problems that benefit from trial-and-error and learning from feedback.
- Generating and refining code based on test results, compiler output, or linter feedback.
- Enhancing sequential decision-making in dynamic environments where feedback is available.
- Improving reasoning capabilities by reflecting on evaluation outcomes and adjusting strategy.
- Automating processes that involve learning from explicit feedback signals (e.g., scores, error messages).
- Creative writing or content generation with iterative improvement based on critiques or specified criteria.

## Parameters for the `reflexion-thinking` tool

The `reflexion-thinking` tool accepts the following parameters in its `arguments` object:

-   `stepType` (string, enum: `"actor"`, `"evaluator"`, `"self-reflection"`): The current step in the Reflexion process. **(Required)**
-   `trialNumber` (integer): Current trial number (minimum: 1). **(Required)**
-   `maxTrials` (integer): Total number of trials planned for the task (minimum: 1). **(Required)**
-   `actorInputText` (string, optional): The initial prompt or task description for the Actor.
    -   Required for `stepType: "actor"`.
-   `actorOutputText` (string, optional): The output generated by the Actor in the current trial.
    -   Required for `stepType: "evaluator"`.
    -   Required for `stepType: "self-reflection"`.
-   `evaluatorScore` (string | number, optional): The evaluation score or textual feedback for the Actor's output.
    -   Required for `stepType: "self-reflection"`.
-   `reflectionText` (string, optional): The verbal reflection text.
    -   Typically provided when `stepType: "self-reflection"` to complete the trial's reflection phase.
    -   If not provided during a 'self-reflection' call (when `actorOutputText` and `evaluatorScore` are present), the server will output a prompt to help an LLM generate this `reflectionText`.
-   `memoryOverride` (array of strings, optional): Allows providing an initial list of reflections to populate the agent's memory, ordered from most to least recent.

## Reflexion Step Guidelines

The Reflexion process is cyclical and involves multiple calls to the `reflexion-thinking` tool.

1.  **`actor` step:**
    *   **Goal:** Generate an initial output for the task.
    *   **Input:** Provide `stepType: "actor"`, `trialNumber`, `maxTrials`, and `actorInputText` (the task or problem).
    *   **Output:** The server returns a structure including `prompt_for_actor` (which contains the `actorInputText`) and `current_memory`. This is intended to be passed to an LLM (the Actor) to generate the actual `actorOutputText`.
    *   **Next:** Call the tool again with `stepType: "evaluator"`, the original `trialNumber` and `maxTrials`, and the LLM-generated `actorOutputText`.

2.  **`evaluator` step:**
    *   **Goal:** Evaluate the Actor's output.
    *   **Input:** Provide `stepType: "evaluator"`, `trialNumber`, `maxTrials`, and `actorOutputText` (the output from the Actor).
    *   **Output:** The server returns a structure including `content_to_evaluate` (which is the `actorOutputText`). This prompts the user (or an Evaluator LLM) to assess the output and produce an `evaluatorScore`.
    *   **Next:** Call the tool again with `stepType: "self-reflection"`, the original `trialNumber`, `maxTrials`, the `actorOutputText`, and the determined `evaluatorScore`.

3.  **`self-reflection` step:**
    This step typically involves two calls if an LLM is used to generate the reflection:
    *   **Call 1 (To get a prompt for the Reflection LLM):**
        *   **Goal:** Prepare a prompt for an LLM to generate a reflection.
        *   **Input:** Provide `stepType: "self-reflection"`, `trialNumber`, `maxTrials`, `actorOutputText`, and `evaluatorScore`.
        *   **Output:** The server returns a structure including `prompt_for_reflection_llm`. This prompt contains the `actorOutputText`, `evaluatorScore`, and `current_memory` to guide a Self-Reflection LLM in generating `reflectionText`.
        *   **Next:** Use an LLM to generate `reflectionText` based on this prompt.
    *   **Call 2 (To store the reflection and complete the trial):**
        *   **Goal:** Store the generated reflection, update memory, and conclude the trial.
        *   **Input:** Provide `stepType: "self-reflection"`, `trialNumber`, `maxTrials`, `actorOutputText`, `evaluatorScore`, AND the newly generated `reflectionText`.
        *   **Output:** The server adds the `reflectionText` to its internal `memory`, logs the trial data in `trialHistory`, and returns a structure indicating `trialCompleted`, the updated `memory`, and whether a `next_trial_needed` (if `trialNumber < maxTrials`).
        *   **Next (if `next_trial_needed` is true):** Increment `trialNumber` and go back to the `actor` step for the new trial.

## How to use

To use this server with an MCP client (like GLAMA), you would configure it in your client's settings. The command below assumes you have `npx` and the package `mcp-server-reflexion-thinking` (replace with actual package name if different) available.

```json
{
  "mcpServers": {
    "reflexion-thinking": {
      "command": "npx",
      "args": ["-y", "mcp-server-reflexion-thinking"]
    }
  }
}
```
*(Note: Replace `"mcp-server-reflexion-thinking"` with the actual published package name if it differs.)*

## Example Usage Flow

**Scenario:** A simple code generation task: "Write a Python function that takes two numbers and returns their sum." We'll aim for 3 trials.

**Trial 1**

1.  **Actor Step (Client to Server):**
    ```json
    {
      "stepType": "actor",
      "trialNumber": 1,
      "maxTrials": 3,
      "actorInputText": "Write a Python function that takes two numbers and returns their sum."
    }
    ```
    *Server responds with a structure for an LLM (Actor) to generate the function.*

2.  **LLM (Actor) generates code (Client-side):**
    `actorOutputText = "def add(a, b): return a - b"` (Intentional error for demonstration)

3.  **Evaluator Step (Client to Server):**
    ```json
    {
      "stepType": "evaluator",
      "trialNumber": 1,
      "maxTrials": 3,
      "actorOutputText": "def add(a, b): return a - b"
    }
    ```
    *Server responds, prompting for evaluation of the `actorOutputText`.*

4.  **User/LLM (Evaluator) provides score (Client-side):**
    `evaluatorScore = "Test failed: add(2, 3) returned -1, expected 5."`

5.  **Self-Reflection Step - Call 1 (Client to Server - To get reflection prompt):**
    ```json
    {
      "stepType": "self-reflection",
      "trialNumber": 1,
      "maxTrials": 3,
      "actorOutputText": "def add(a, b): return a - b",
      "evaluatorScore": "Test failed: add(2, 3) returned -1, expected 5."
    }
    ```
    *Server responds with `prompt_for_reflection_llm` containing the context for a Self-Reflection LLM.*

6.  **LLM (Self-Reflection) generates reflection (Client-side):**
    `reflectionText = "The function used subtraction instead of addition. I need to change '-' to '+'."`

7.  **Self-Reflection Step - Call 2 (Client to Server - To store reflection):**
    ```json
    {
      "stepType": "self-reflection",
      "trialNumber": 1,
      "maxTrials": 3,
      "actorOutputText": "def add(a, b): return a - b",
      "evaluatorScore": "Test failed: add(2, 3) returned -1, expected 5.",
      "reflectionText": "The function used subtraction instead of addition. I need to change '-' to '+'."
    }
    ```
    *Server adds reflection to memory. Returns `next_trial_needed: true`.*

**Trial 2**

1.  **Actor Step (Client to Server):**
    ```json
    {
      "stepType": "actor",
      "trialNumber": 2,
      "maxTrials": 3,
      "actorInputText": "Write a Python function that takes two numbers and returns their sum."
    }
    ```
    *Server responds. The prompt for the Actor LLM will now include the reflection from Trial 1 in its memory.*

2.  **LLM (Actor) generates code (Client-side):**
    (Hopefully, using the reflection in memory: `The function used subtraction instead of addition. I need to change '-' to '+'.`)
    `actorOutputText = "def add(a, b): return a + b"`

...and so on. The process continues with evaluation and potential further reflection until `maxTrials` is reached or the output is satisfactory.

Here's a specific example in the domain of geometric algebra:

### Example 1 · Translating a Projective Conic into Conformal Geometric Algebra

> Task
> Translate the projective conic
> C : x² + y² − z² = 0
> into conformal geometric-algebra (CGA) representation.

<details>
<summary>Click to view Reflexion trace</summary>

```diff
@@ Actor (Initial attempt)
- The conic x² + y² − z² = 0 can be viewed as a circle in projective space.
- In geometric algebra we might write it as a bivector B = e₁ ∧ e₂,
- corresponding to the plane of the circle.

@@ Evaluator (Feedback)
+ Too vague: a circle in CGA is a **trivector** (outer product of three conformal points).
+ Need to embed points into ℝ^{4,1} first and be explicit about null vectors.
+ Clarify projective coordinates vs. Euclidean slice.

@@ Actor (After reflection)
+ Embed ℝ² into CGA with null vectors e₊, e₋:
+   P(x,y) = x e₁ + y e₂ + ½(x² + y²) e₊ + e₋.
+ Choose three points on the conic, e.g.
+   P₁ = P(1,0),  P₂ = P(0,1),  P₃ = P(1,1).
+ The circle is the trivector
+   C = P₁ ∧ P₂ ∧ P₃.
+ This blade encodes the same locus as x² + y² − z² = 0 under the conformal embedding.
```

Takeaway. The reflection fixes two key gaps: (1) moves from a bivector to the correct trivector representation, and (2) makes the ℝ^{4,1}     embedding explicit.
</details>

## Best Practices

1.  **Clearly define the task** for the Actor in `actorInputText`. The more specific the task, the better the Actor can perform.
2.  Ensure the Evaluator provides **specific and accurate** `evaluatorScore` or feedback. Vague feedback is less helpful for reflection.
3.  When generating `reflectionText` (usually via an LLM), focus on **actionable insights** derived from the `evaluatorScore` and `actorOutputText`. What specifically went wrong, and how can it be fixed or improved next time?
4.  Iterate through trials (`trialNumber`) up to `maxTrials`, allowing the memory of reflections to guide improvement. Don't expect perfection in the first trial.
5.  For the **`self-reflection` step, make two calls if an LLM generates the reflection**: the first to get the prompt (by omitting `reflectionText`), the second to provide the LLM-generated `reflectionText`.
6.  Use `memoryOverride` if you need to bootstrap the agent with pre-existing knowledge or a specific set of initial reflections, for example, to guide its first attempt in a particular direction.
7.  The quality of the LLMs used for Actor, Evaluator (if applicable), and Self-Reflection significantly impacts the overall performance of the Reflexion process.

---

This README provides an overview of the Reflexion Thinking MCP Server. For more details on the underlying Reflexion methodology, please refer to the paper: *Shinn, N., Cassano, F., Berman, E., Gopinath, A., Narasimhan, K., & Yao, S. (2023). Reflexion: Language Agents with Verbal Reinforcement Learning.*
